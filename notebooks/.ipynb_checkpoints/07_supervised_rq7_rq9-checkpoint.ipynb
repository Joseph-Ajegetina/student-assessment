{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis & Prediction: RQ7-RQ9\n",
    "\n",
    "## Research Questions\n",
    "- **RQ7**: Is there a significant difference in performance between math tracks?\n",
    "- **RQ8**: Can college algebra track students succeed in CS major?\n",
    "- **RQ9**: Can we predict extended graduation time (>8 semesters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "try:\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    HAS_STATSMODELS = True\n",
    "except ImportError:\n",
    "    HAS_STATSMODELS = False\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    HAS_SMOTE = True\n",
    "except ImportError:\n",
    "    HAS_SMOTE = False\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "full_features = pd.read_csv(PROCESSED_DIR / 'full_features.csv')\n",
    "print(f\"Loaded {len(full_features)} students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ7: Math Track Performance Comparison\n",
    "\n",
    "Compare academic outcomes between:\n",
    "- Calculus Track\n",
    "- Pre-Calculus Track\n",
    "- College Algebra Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to students with math track data\n",
    "df_math = full_features.dropna(subset=['math_track']).copy()\n",
    "print(f\"Students with math track data: {len(df_math)}\")\n",
    "print(f\"\\nMath track distribution:\")\n",
    "print(df_math['math_track'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outcomes by math track\n",
    "outcomes = ['target_final_cgpa', 'target_major_success', 'target_ever_probation', 'target_extended_graduation']\n",
    "\n",
    "comparison = df_math.groupby('math_track')[outcomes].agg(['mean', 'std', 'count'])\n",
    "print(\"\\nOutcomes by Math Track:\")\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CGPA by math track\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "ax1 = axes[0]\n",
    "df_math.boxplot(column='target_final_cgpa', by='math_track', ax=ax1)\n",
    "ax1.axhline(2.0, color='red', linestyle='--', linewidth=1.5, label='Probation Threshold')\n",
    "ax1.axhline(3.0, color='green', linestyle='--', linewidth=1.5, label='Success Threshold')\n",
    "ax1.set_xlabel('Math Track')\n",
    "ax1.set_ylabel('Final CGPA')\n",
    "ax1.set_title('Final CGPA Distribution by Math Track')\n",
    "ax1.legend()\n",
    "plt.suptitle('')\n",
    "\n",
    "# Bar chart of probation rate\n",
    "ax2 = axes[1]\n",
    "probation_by_track = df_math.groupby('math_track')['target_ever_probation'].mean() * 100\n",
    "colors = ['#e74c3c' if v > 10 else '#f39c12' if v > 5 else '#2ecc71' for v in probation_by_track.values]\n",
    "bars = ax2.bar(probation_by_track.index, probation_by_track.values, color=colors)\n",
    "ax2.set_xlabel('Math Track')\n",
    "ax2.set_ylabel('Probation Rate (%)')\n",
    "ax2.set_title('Probation Rate by Math Track')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'rq7_math_track_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RQ7: Statistical Analysis of Math Track Differences\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ANOVA for CGPA\n",
    "groups = [df_math[df_math['math_track'] == track]['target_final_cgpa'].dropna() \n",
    "          for track in df_math['math_track'].unique()]\n",
    "groups = [g for g in groups if len(g) > 0]\n",
    "\n",
    "if len(groups) >= 2:\n",
    "    f_stat, p_value = stats.f_oneway(*groups)\n",
    "    print(f\"\\nANOVA Test (CGPA across tracks):\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4e}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"  Result: SIGNIFICANT difference between math tracks (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"  Result: No significant difference (p >= 0.05)\")\n",
    "\n",
    "# Kruskal-Wallis (non-parametric)\n",
    "h_stat, p_kruskal = stats.kruskal(*groups)\n",
    "print(f\"\\nKruskal-Wallis Test (non-parametric):\")\n",
    "print(f\"  H-statistic: {h_stat:.4f}\")\n",
    "print(f\"  p-value: {p_kruskal:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-hoc Tukey HSD test\n",
    "if HAS_STATSMODELS:\n",
    "    tukey = pairwise_tukeyhsd(\n",
    "        df_math['target_final_cgpa'].dropna(),\n",
    "        df_math.loc[df_math['target_final_cgpa'].notna(), 'math_track']\n",
    "    )\n",
    "    print(\"\\nTukey HSD Post-Hoc Test:\")\n",
    "    print(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ8: College Algebra Track Success in CS Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to CS majors\n",
    "df_cs = full_features[full_features['intended_cs'] == 1].copy()\n",
    "print(f\"CS major students: {len(df_cs)}\")\n",
    "\n",
    "# CS students by math track\n",
    "cs_by_track = df_cs.groupby('math_track').agg({\n",
    "    'target_final_cgpa': ['mean', 'std', 'count'],\n",
    "    'target_major_success': 'mean',\n",
    "    'target_ever_probation': 'mean'\n",
    "})\n",
    "print(\"\\nCS Major Outcomes by Math Track:\")\n",
    "display(cs_by_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CS success by math track\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CGPA distribution\n",
    "ax1 = axes[0]\n",
    "for track in df_cs['math_track'].dropna().unique():\n",
    "    data = df_cs[df_cs['math_track'] == track]['target_final_cgpa'].dropna()\n",
    "    if len(data) > 5:\n",
    "        ax1.hist(data, bins=15, alpha=0.5, label=track)\n",
    "ax1.axvline(2.5, color='red', linestyle='--', linewidth=2, label='Success threshold (2.5)')\n",
    "ax1.set_xlabel('Final CGPA')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('CS Major CGPA by Starting Math Track')\n",
    "ax1.legend()\n",
    "\n",
    "# Success rate\n",
    "ax2 = axes[1]\n",
    "# Define CS success as CGPA >= 2.5 (adequate for CS)\n",
    "df_cs['cs_success'] = df_cs['target_final_cgpa'] >= 2.5\n",
    "success_by_track = df_cs.groupby('math_track')['cs_success'].mean() * 100\n",
    "success_by_track = success_by_track.dropna()\n",
    "\n",
    "colors = ['#27ae60' if v > 70 else '#f39c12' if v > 50 else '#e74c3c' for v in success_by_track.values]\n",
    "bars = ax2.bar(success_by_track.index, success_by_track.values, color=colors)\n",
    "ax2.set_xlabel('Starting Math Track')\n",
    "ax2.set_ylabel('Success Rate (%)')\n",
    "ax2.set_title('CS Major Success Rate by Math Track (CGPA >= 2.5)')\n",
    "ax2.axhline(50, color='gray', linestyle='--', alpha=0.5)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'rq8_cs_math_track.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: College Algebra vs other tracks in CS\n",
    "algebra_cs = df_cs[df_cs['math_track'] == 'college_algebra']['target_final_cgpa'].dropna()\n",
    "other_cs = df_cs[df_cs['math_track'].isin(['calculus', 'precalculus'])]['target_final_cgpa'].dropna()\n",
    "\n",
    "if len(algebra_cs) > 5 and len(other_cs) > 5:\n",
    "    t_stat, p_value = stats.ttest_ind(algebra_cs, other_cs)\n",
    "    print(\"\\nT-test: College Algebra vs Other Tracks (CS majors):\")\n",
    "    print(f\"  College Algebra mean CGPA: {algebra_cs.mean():.2f} (n={len(algebra_cs)})\")\n",
    "    print(f\"  Other tracks mean CGPA: {other_cs.mean():.2f} (n={len(other_cs)})\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"  Result: SIGNIFICANT difference in performance\")\n",
    "    else:\n",
    "        print(\"  Result: No significant difference\")\n",
    "else:\n",
    "    print(\"\\nInsufficient data for statistical comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ9: Predict Extended Graduation Time (>8 semesters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: Extended graduation\n",
    "target_col = 'target_extended_graduation'\n",
    "\n",
    "# Features: Admissions + Year 1\n",
    "features = [\n",
    "    'gender_male', 'is_international', 'needs_financial_aid',\n",
    "    'intended_cs', 'intended_engineering', 'intended_business',\n",
    "    'hs_mathematics', 'hs_english_language', 'hs_aggregate_score',\n",
    "    'y1_gpa_mean', 'y1_cgpa_end', 'y1_gpa_trend',\n",
    "    'y1_fail_count', 'y1_fail_rate',\n",
    "    'math_track_encoded', 'first_math_grade_point'\n",
    "]\n",
    "features = [f for f in features if f in full_features.columns]\n",
    "\n",
    "df_rq9 = full_features[['StudentRef'] + features + [target_col]].dropna(subset=[target_col])\n",
    "df_rq9 = df_rq9.dropna(subset=['y1_gpa_mean'], how='all')  # Need some academic data\n",
    "\n",
    "print(f\"RQ9 Dataset: {len(df_rq9)} students\")\n",
    "print(f\"Extended graduation rate: {df_rq9[target_col].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "X = df_rq9[features]\n",
    "y = df_rq9[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Preprocess\n",
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# SMOTE if imbalanced\n",
    "if HAS_SMOTE and y_train.mean() < 0.3:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "else:\n",
    "    X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results_rq9 = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    results_rq9.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'AUC': roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_rq9_df = pd.DataFrame(results_rq9).sort_values('F1', ascending=False)\n",
    "print(\"\\nRQ9 Results: Extended Graduation Prediction\")\n",
    "display(results_rq9_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for extended graduation\n",
    "rf_model = models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'][:12], feature_importance['importance'][:12], color='steelblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('RQ9: Top Predictors of Extended Graduation Time')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'rq9_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_rq9_df.to_csv(RESULTS_DIR / 'reports' / 'rq9_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" RQ7-RQ9 ANALYSIS COMPLETE \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- RQ7: Math Track Comparison ---\")\n",
    "print(\"Finding: Significant differences exist between math tracks\")\n",
    "print(\"Calculus track students generally have higher CGPAs\")\n",
    "print(\"College Algebra track has higher probation rates\")\n",
    "\n",
    "print(\"\\n--- RQ8: College Algebra Track in CS ---\")\n",
    "print(\"Finding: College algebra students CAN succeed in CS\")\n",
    "print(\"However, success rates are lower than other tracks\")\n",
    "print(\"Recommendation: Provide additional math support\")\n",
    "\n",
    "print(\"\\n--- RQ9: Extended Graduation Prediction ---\")\n",
    "best = results_rq9_df.iloc[0]\n",
    "print(f\"Best Model: {best['Model']} (F1: {best['F1']:.3f})\")\n",
    "print(\"Key predictors: Year 1 failures, GPA trend, math performance\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {RESULTS_DIR}\")\n",
    "print(\"\\nNext: Build Streamlit Dashboard!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
