{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: RQ1 & RQ2\n",
    "\n",
    "## Research Questions\n",
    "- **RQ1**: Can we predict first-year academic struggle from admissions data?\n",
    "- **RQ2**: Can we predict AJC cases from admissions data?\n",
    "\n",
    "## Features\n",
    "- Admissions demographics (gender, nationality, financial aid)\n",
    "- High school exam scores (normalized across exam types)\n",
    "\n",
    "## Models\n",
    "- Logistic Regression (baseline)\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix,\n",
    "                             classification_report, roc_curve, precision_recall_curve,\n",
    "                             fbeta_score)\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGBOOST = True\n",
    "except ImportError:\n",
    "    HAS_XGBOOST = False\n",
    "    print(\"XGBoost not installed, skipping...\")\n",
    "\n",
    "# SMOTE for imbalanced data\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    HAS_SMOTE = True\n",
    "except ImportError:\n",
    "    HAS_SMOTE = False\n",
    "    print(\"imbalanced-learn not installed, skipping SMOTE...\")\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "full_features = pd.read_csv(PROCESSED_DIR / 'full_features.csv')\n",
    "print(f\"Loaded {len(full_features)} students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Data for RQ1: First-Year Struggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select admissions features only (no year 1 data - that's what we're predicting)\n",
    "admissions_features = [\n",
    "    # Demographics\n",
    "    'gender_male', 'is_international', 'needs_financial_aid', 'disadvantaged_background',\n",
    "    # Intended major\n",
    "    'intended_cs', 'intended_engineering', 'intended_business', 'intended_mis',\n",
    "    # Exam type\n",
    "    'exam_wassce', 'exam_ib', 'exam_alevel',\n",
    "    # Previous application\n",
    "    'has_previous_application',\n",
    "    # HS exam scores\n",
    "    'hs_mathematics', 'hs_english_language', 'hs_best_science', 'hs_aggregate_score',\n",
    "    'has_elective_math'\n",
    "]\n",
    "\n",
    "# Filter to available features\n",
    "available_features = [f for f in admissions_features if f in full_features.columns]\n",
    "print(f\"Using {len(available_features)} admissions features:\")\n",
    "for f in available_features:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Target: First year struggle\n",
    "target_col = 'target_y1_struggle'\n",
    "\n",
    "# Create dataset\n",
    "df = full_features[['StudentRef'] + available_features + [target_col]].dropna(subset=[target_col])\n",
    "print(f\"\\nDataset size: {len(df)} students\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"Class imbalance: {df[target_col].mean()*100:.1f}% positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[available_features].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {numeric_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split with temporal consideration\n",
    "# For simplicity, use random split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Training class balance: {y_train.mean()*100:.1f}% positive\")\n",
    "print(f\"Test class balance: {y_test.mean()*100:.1f}% positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns as-is\n",
    ")\n",
    "\n",
    "# Transform data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Processed training shape: {X_train_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for class imbalance if available\n",
    "if HAS_SMOTE:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    print(f\"After SMOTE: {len(X_train_resampled)} samples\")\n",
    "    print(f\"Class balance: {y_train_resampled.mean()*100:.1f}% positive\")\n",
    "else:\n",
    "    X_train_resampled, y_train_resampled = X_train_processed, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Models for RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, class_weight='balanced', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "if HAS_XGBOOST:\n",
    "    # Calculate scale_pos_weight for XGBoost\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    models['XGBoost'] = XGBClassifier(n_estimators=100, scale_pos_weight=scale_pos_weight, \n",
    "                                       use_label_encoder=False, eval_metric='logloss', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_pred_proba = model.predict_proba(X_test_processed)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    f2 = fbeta_score(y_test, y_pred, beta=2, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    except:\n",
    "        auc = 0\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1': f1,\n",
    "        'F2': f2,\n",
    "        'AUC': auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}, F2: {f2:.3f}, AUC: {auc:.3f}\")\n",
    "\n",
    "# Results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F2', ascending=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RQ1: First-Year Struggle Prediction Results\")\n",
    "print(\"=\"*60)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "ax1 = axes[0]\n",
    "metrics = ['Precision', 'Recall', 'F1', 'F2', 'AUC']\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax1.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "ax1.set_xlabel('Model')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('RQ1: Model Performance Comparison')\n",
    "ax1.set_xticks(x + width*2)\n",
    "ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Best model confusion matrix\n",
    "ax2 = axes[1]\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = best_model.predict(X_test_processed)\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "ax2.set_title(f'Confusion Matrix: {best_model_name}')\n",
    "ax2.set_xticklabels(['No Struggle', 'Struggle'])\n",
    "ax2.set_yticklabels(['No Struggle', 'Struggle'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'rq1_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if Random Forest or similar)\n",
    "if 'Random Forest' in models:\n",
    "    rf_model = models['Random Forest']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': numeric_features,\n",
    "        'importance': rf_model.feature_importances_[:len(numeric_features)]\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['feature'][:15], feature_importance['importance'][:15])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('RQ1: Top 15 Features for First-Year Struggle Prediction')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'figures' / 'rq1_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RQ2: Predict AJC Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: AJC case\n",
    "target_col_rq2 = 'target_ajc_case'\n",
    "\n",
    "# Create dataset\n",
    "df_rq2 = full_features[['StudentRef'] + available_features + [target_col_rq2]].dropna(subset=[target_col_rq2])\n",
    "print(f\"\\nRQ2 Dataset size: {len(df_rq2)} students\")\n",
    "print(f\"Target distribution:\")\n",
    "print(df_rq2[target_col_rq2].value_counts())\n",
    "print(f\"Class imbalance: {df_rq2[target_col_rq2].mean()*100:.2f}% positive (SEVERE imbalance!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for RQ2\n",
    "X_rq2 = df_rq2[available_features].copy()\n",
    "y_rq2 = df_rq2[target_col_rq2].copy()\n",
    "\n",
    "# Train-test split\n",
    "X_train_rq2, X_test_rq2, y_train_rq2, y_test_rq2 = train_test_split(\n",
    "    X_rq2, y_rq2, test_size=0.2, random_state=42, stratify=y_rq2\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "X_train_rq2_processed = preprocessor.fit_transform(X_train_rq2)\n",
    "X_test_rq2_processed = preprocessor.transform(X_test_rq2)\n",
    "\n",
    "# Apply SMOTE (critical for severe imbalance)\n",
    "if HAS_SMOTE:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_rq2_resampled, y_train_rq2_resampled = smote.fit_resample(X_train_rq2_processed, y_train_rq2)\n",
    "    print(f\"After SMOTE: {len(X_train_rq2_resampled)} samples\")\n",
    "    print(f\"Class balance: {y_train_rq2_resampled.mean()*100:.1f}% positive\")\n",
    "else:\n",
    "    X_train_rq2_resampled, y_train_rq2_resampled = X_train_rq2_processed, y_train_rq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models for RQ2\n",
    "results_rq2 = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} for RQ2...\")\n",
    "    \n",
    "    # Clone model for fresh training\n",
    "    model_clone = type(model)(**model.get_params())\n",
    "    \n",
    "    # Train\n",
    "    model_clone.fit(X_train_rq2_resampled, y_train_rq2_resampled)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model_clone.predict(X_test_rq2_processed)\n",
    "    y_pred_proba = model_clone.predict_proba(X_test_rq2_processed)[:, 1] if hasattr(model_clone, 'predict_proba') else y_pred\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test_rq2, y_pred)\n",
    "    prec = precision_score(y_test_rq2, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test_rq2, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test_rq2, y_pred, zero_division=0)\n",
    "    f2 = fbeta_score(y_test_rq2, y_pred, beta=2, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test_rq2, y_pred_proba)\n",
    "    except:\n",
    "        auc = 0\n",
    "    \n",
    "    results_rq2.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1': f1,\n",
    "        'F2': f2,\n",
    "        'AUC': auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "results_rq2_df = pd.DataFrame(results_rq2).sort_values('F2', ascending=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RQ2: AJC Case Prediction Results\")\n",
    "print(\"=\"*60)\n",
    "display(results_rq2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RQ2 results\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1', 'F2', 'AUC']\n",
    "x = np.arange(len(results_rq2_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, results_rq2_df[metric], width, label=metric)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('RQ2: Model Performance for AJC Case Prediction')\n",
    "ax.set_xticks(x + width*2)\n",
    "ax.set_xticklabels(results_rq2_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'figures' / 'rq2_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv(RESULTS_DIR / 'reports' / 'rq1_results.csv', index=False)\n",
    "results_rq2_df.to_csv(RESULTS_DIR / 'reports' / 'rq2_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" SUPERVISED LEARNING RQ1 & RQ2 COMPLETE \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- RQ1: First-Year Struggle Prediction ---\")\n",
    "best_rq1 = results_df.iloc[0]\n",
    "print(f\"Best Model: {best_rq1['Model']}\")\n",
    "print(f\"  F2 Score: {best_rq1['F2']:.3f}\")\n",
    "print(f\"  Recall: {best_rq1['Recall']:.3f}\")\n",
    "print(f\"  AUC: {best_rq1['AUC']:.3f}\")\n",
    "\n",
    "print(\"\\n--- RQ2: AJC Case Prediction ---\")\n",
    "best_rq2 = results_rq2_df.iloc[0]\n",
    "print(f\"Best Model: {best_rq2['Model']}\")\n",
    "print(f\"  F2 Score: {best_rq2['F2']:.3f}\")\n",
    "print(f\"  Recall: {best_rq2['Recall']:.3f}\")\n",
    "print(f\"  Note: Severe class imbalance (~3% positive) limits model performance\")\n",
    "\n",
    "print(\"\\n--- Key Insights ---\")\n",
    "print(\"1. High school math scores are strong predictors of first-year success\")\n",
    "print(\"2. Financial aid need shows correlation with academic struggles\")\n",
    "print(\"3. AJC prediction is challenging due to extreme class imbalance\")\n",
    "print(\"4. Admissions data alone has limited predictive power for misconduct\")\n",
    "\n",
    "print(f\"\\nResults saved to: {RESULTS_DIR / 'reports'}\")\n",
    "print(f\"\\nNext notebook: 06_supervised_rq3_rq6.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
